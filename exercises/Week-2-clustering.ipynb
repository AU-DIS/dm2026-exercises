{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 (K-means, K-medoids, Gaussian Mixtures)\n",
    "\n",
    "This week we are going to work with K-means, K-medoids, and Gaussian Mixtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Local imports (used for the last optional exercise.)\n",
    "import math\n",
    "import itertools\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from utilities.load_data import load_iris, load_iris_PC, index_to_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Warmup\n",
    "Please provide a brief description of what characterises \n",
    "1. Clustering as a task \n",
    "2. Representative-based clustering as a clustering approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Practical K-means\n",
    "Given the following points: 2, 4, 10, 12, 3, 20, 30, 11, 25. Assume $k=3$, and that we randomly pick the initial means $\\mu_1=2$, $ \\mu_2=4$ and $\\mu_3=6$. Show the clusters obtained using K-means algorithm after one iteration, and show the new means for the next iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can use python if you want, but it is not required!\n",
    "X = np.array([2, 4, 10, 12, 3, 20, 30, 11, 25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "Which algorithm is more robust: k-means or k-medoid and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Practical Mixture of Gaussians\n",
    "Given the data points in table below, and their probability of belonging to two clusters.\n",
    "Assume that these points were produced by a mixture of two univariate normal distributions. \n",
    "Answer the following questions:\n",
    "\n",
    "1. Find the maximum likelihood estimate of the means $\\mu_1$ and $\\mu_2$\n",
    "2. Assume that $\\mu_1 = 2$, $\\mu_2 = 7$, and $\\sigma_1 = \\sigma_2 = 1$. Find the probability that the point $x=5$ belongs to cluster $C_1$ and to cluster $C_2$. You may assume that the prior probability of each cluster is equal (i.e., $P(C_1) = P(C_2) = 0.5$), and the prior probability $P(x=5) = 0.029$\n",
    "\n",
    "|$x$|$P(C_1\\mid x)$|$P(C_2\\mid x)$|\n",
    "|:---:|:---:|:---:|\n",
    "| --- | ---------------- | ---------------- |\n",
    "|2 |  0.9  |  0.1  |\n",
    "|3|0.8|0.2|\n",
    "|7|0.3|0.7|\n",
    "|9|0.1|0.9|\n",
    "|2|0.9|0.1|\n",
    "|1|0.8|0.2|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want, you can use python here.\n",
    "X            = np.array([2, 3, 7, 9, 2, 1]).reshape(-1,1) # Reshaped from (6,) to (6,1)\n",
    "P_C1_given_x = np.array([0.9, 0.8, 0.3, 0.1, 0.9, 0.8])\n",
    "P_C2_given_x = 1 - P_C1_given_x\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "For which parameter $\\mu,\\Sigma,P(C)$ settings is EM clustering identical to k-means clustering and why?<br>\n",
    "Mean $\\mu_i = \\frac{{\\sum_{j=1}^{n}{x_j} w_{ij}}}{\\sum_{j=1}^{n}{w_{ij}}}$<br>\n",
    "Covariance $\\Sigma_i = \\frac{\\sum_{j=1}^{n}w_{ij}(x_j - \\mu_i)(x_j - \\mu_i)^\\top}{\\sum_{j=1}^{n} w_{ij}}$<br>\n",
    "Prior $P(c_i) = \\frac{\\sum_{j=1}^{n}w_{ij}}{n}$<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Exercise 6: 2d K-means and gaussian mixture\n",
    "Given the two-dimensional points in Table 13.2, assume that $k=2$, and that initially the points are assigned to clusters as follors: $C_1 = \\{ x_1, x_2, x_4 \\}$ and $C_2 = \\{ x_3, x_5 \\}$.\n",
    "Answer the following questions:\n",
    "\n",
    "1. Apply the K-means algorithm until convergence, that is, the clusters do not change, assuming (1) the usual Euclidean distance of the $L_2$-norm as the distance between points, defined as\n",
    "\n",
    "$$\n",
    "||x_i - x_j||_2 = \\sqrt{ \\sum_{a=1}^d (x_{ia} - x_{ja})^2 }\n",
    "$$\n",
    " and (2) the Manhattan distance of the $L_1$-norm\n",
    "$$\n",
    "||x_i - x_j||_1 = \\sum_{a=1}^d |x_{ia} - x_{ja}|.\n",
    "$$\n",
    "\n",
    "2. Apply the EM algorithm with $k=2$ assuming that the dimensions are independent. Show one complete execution of the expectation and the maximization steps. Start with the assumption that $P(C_i | x_{ja}) = 0.5$ for $a=1, 2$ and $j=1, ..., 5$.\n",
    "\n",
    "\n",
    "![Table 13.2](graphics/13.2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, if you want, you can use a bit of Python\n",
    "X = np.array([\n",
    "    [0, 0, 1.5, 5, 5],\n",
    "    [2, 0,   0, 0, 2]\n",
    "]).T # shape [5, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optionals\n",
    "## Exercise 7\n",
    "Consider 2D data (2,2), (2,1), (2,3), (1,2), (3,2), (8,2), (8,1), (8,0), (8,3), (8,4), (7,2), (6,2), (9,2), (10,2), (7,1), (7,3), (9,1), (9,3)  \n",
    "\n",
    "![Data plotted](graphics/two_cluster_dataplot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. let k=2 and sketch visually what you think the final clustering will be and explain why. \n",
    "2. Does the initialization influence the final clustering? And why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8: K-means and the Iris dataset\n",
    "\n",
    "In this exercise, we will apply K-means to the two 2PC dataset from [Zaki] (and slides from Week 2).\n",
    "You may use the code below as inspiration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_iris_PC()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].scatter(*(X.T), c=y)\n",
    "ax[0].set_title(\"Real clusters\")\n",
    "\n",
    "def kmeans(X, k):\n",
    "    \"\"\"\n",
    "        Arguments:\n",
    "            k: int specifying number of clusters\n",
    "            X: numpy array with data\n",
    "        Returns:\n",
    "            clusters: Array of indicators (ints) indicating the cluster of each point. Shape: [n,]\n",
    "    \"\"\"\n",
    "    n, d = X.shape\n",
    "    clusters = None \n",
    "    new_clusters = None\n",
    "    centroids = np.random.randn(k, d) # K clusters of shape d\n",
    "    \n",
    "    while clusters is None or (not np.allclose(clusters, new_clusters)):\n",
    "        clusters = None # TODO assign points to clusters\n",
    "        centroids = None # TODO reassign centroids\n",
    "        break # Remove this. It is to prevent you getting an infinite loop if you try to run the code before modifying it\n",
    "        \n",
    "    return clusters\n",
    "\n",
    "clusters = kmeans(X, 3)\n",
    "ax[1].scatter(*(X.T), c=clusters)\n",
    "ax[1].set_title(\"K-means clusters\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dm25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
